name: Tests

permissions:
  contents: read

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.github/workflows/docs.yml'
  pull_request:
    branches: [ main, develop ]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.github/workflows/docs.yml'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: stable

    - name: Download dependencies
      run: go mod download

    - name: Make test script executable
      run: chmod +x scripts/run-tests.sh

    - name: Run unit tests
      run: ./scripts/run-tests.sh unit

    - name: Upload coverage reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-reports
        path: |
          coverage.out
          coverage.html
        retention-days: 7

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: stable

    - name: Download dependencies
      run: go mod download

    - name: Make test script executable
      run: chmod +x scripts/run-tests.sh

    - name: Run integration tests
      env:
        NETBIRD_API_TOKEN: ${{ secrets.NETBIRD_API_TOKEN }}
      run: ./scripts/run-tests.sh integration

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: stable

    - name: Download dependencies
      run: go mod download

    - name: Make test script executable
      run: chmod +x scripts/run-tests.sh

    - name: Run performance tests
      run: ./scripts/run-tests.sh performance

  benchmark-tests:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: stable

    - name: Download dependencies
      run: go mod download

    - name: Make test script executable
      run: chmod +x scripts/run-tests.sh

    - name: Run benchmark tests
      run: ./scripts/run-tests.sh benchmark

  docker-compose-test:
    name: Docker Compose Test
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Start services
      env:
        NETBIRD_API_TOKEN: ${{ secrets.NETBIRD_API_TOKEN }}
      run: docker compose up -d

    - name: Wait for exporter to be ready
      run: sleep 10

    - name: Check metrics endpoint
      run: |
        curl -sSf http://localhost:8080/metrics

  helm-chart-test:
    name: Helm Chart Test
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
    - uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v4

    - name: Create kind cluster
      uses: helm/kind-action@v1
      with:
        cluster_name: netbird-test

    - name: Build Docker image
      run: |
        docker build -t netbird-api-exporter:test .
        kind load docker-image netbird-api-exporter:test --name netbird-test

    - name: Create values override file
      run: |
        cat > test-values.yaml << EOF
        image:
          repository: netbird-api-exporter
          tag: test
          pullPolicy: Never
        netbird:
          apiToken: test-token
          apiUrl: "https://api.netbird.io"
        EOF

    - name: Bake helm chart
      uses: azure/k8s-bake@v3
      with:
        renderEngine: 'helm'
        helmChart: './charts/netbird-api-exporter/'
        overrideFiles: 'test-values.yaml'
        helm-version: 'latest'
      id: bake

    - name: Deploy to kind cluster
      uses: Azure/k8s-deploy@v5
      with:
        action: deploy
        manifests: ${{ steps.bake.outputs.manifestsBundle }}
        namespace: default

    - name: Wait for deployment
      run: |
        kubectl wait --for=condition=available --timeout=300s deployment/netbird-api-exporter

    - name: Test metrics endpoint
      run: |
        kubectl port-forward deployment/netbird-api-exporter 8080:8080 &
        sleep 5
        curl -sSf http://localhost:8080/metrics | grep -q "netbird_"
        pkill -f "kubectl port-forward" || true

    - name: Check deployment status
      run: |
        kubectl get pods -l app.kubernetes.io/name=netbird-api-exporter
        kubectl describe deployment netbird-api-exporter

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, benchmark-tests, docker-compose-test, helm-chart-test]
    if: always()
    steps:
    - name: Check test results
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Type | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || needs.integration-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Tests | ${{ needs.performance-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Benchmark Tests | ${{ needs.benchmark-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Docker Compose Test | ${{ needs.docker-compose-test.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Helm Chart Test | ${{ needs.helm-chart-test.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [[ "${{ needs.unit-tests.result }}" != "success" ]] || \
           [[ "${{ needs.integration-tests.result }}" != "success" && "${{ needs.integration-tests.result }}" != "skipped" ]] || \
           [[ "${{ needs.performance-tests.result }}" != "success" ]] || \
           [[ "${{ needs.benchmark-tests.result }}" != "success" ]] || \
           [[ "${{ needs.docker-compose-test.result }}" != "success" ]] || \
           [[ "${{ needs.helm-chart-test.result }}" != "success" ]]; then
          echo "❌ Some tests failed! Check the individual job logs for details." >> $GITHUB_STEP_SUMMARY
          exit 1
        else
          echo "✅ All critical tests passed!" >> $GITHUB_STEP_SUMMARY
        fi 